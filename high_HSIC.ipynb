{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High-dimensional HSIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import gamma, ortho_group\n",
    "import pickle\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.metrics import pairwise_kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating non-stationary data according to [Pomann et al. 2016](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4812165/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating distributions over time\n",
    "\n",
    "# normal distribution\n",
    "def gen_data(sample_size, sample_points, sd=np.sqrt(0.25), random_state=None):\n",
    "    \"\"\"    \n",
    "    sample_size : number of function samples\n",
    "    sample_points : observation points\n",
    "    delta : the coefficient of X^3\n",
    "    sd : the standard deviation of the observation noise\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    n_points = len(sample_points)\n",
    "    X = rng.normal(0, np.sqrt(10), (sample_size,1)) * np.sqrt(2) * np.sin(2*np.pi*sample_points) + rng.normal(0, np.sqrt(5), (sample_size,1)) * np.sqrt(2) * np.cos(2*np.pi*sample_points)\n",
    "    X += sample_points    # adding mean function\n",
    "    X += rng.normal(0, sd, (sample_size,n_points))    # adding noise epsilon\n",
    "    return X\n",
    "\n",
    "# uniform\n",
    "def gen_data_uniform(sample_size, sample_points, sd=np.sqrt(0.25), random_state=None):\n",
    "    \"\"\"    \n",
    "    sample_size : number of function samples\n",
    "    sample_points : observation points\n",
    "    delta : the coefficient of X^3\n",
    "    sd : the standard deviation of the observation noise\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    n_points = len(sample_points)\n",
    "    X = rng.uniform(-10, 10, (sample_size,1)) * np.sqrt(2) * np.sin(2*np.pi*sample_points) + rng.uniform(-5, 5, (sample_size,1)) * np.sqrt(2) * np.cos(2*np.pi*sample_points)\n",
    "    X += sample_points    # adding mean function\n",
    "    X += rng.normal(0, sd, (sample_size,n_points))    # adding noise epsilon\n",
    "    return X\n",
    "\n",
    "# exponential\n",
    "def gen_data_exp(sample_size, sample_points, sd=np.sqrt(0.25), random_state=None):\n",
    "    \"\"\"    \n",
    "    sample_size : number of function samples\n",
    "    sample_points : observation points\n",
    "    delta : the coefficient of X^3\n",
    "    sd : the standard deviation of the observation noise\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    n_points = len(sample_points)\n",
    "    X = rng.exponential(1.5, (sample_size,1)) * np.sqrt(2) * np.sin(2*np.pi*sample_points) + rng.exponential(3, (sample_size,1)) * np.sqrt(2) * np.cos(2*np.pi*sample_points)\n",
    "    X += sample_points    # adding mean function\n",
    "    X += rng.normal(0, sd, (sample_size,n_points))    # adding noise epsilon\n",
    "    return X\n",
    "\n",
    "# student t\n",
    "def gen_data_t(sample_size, sample_points, sd=np.sqrt(0.25), random_state=None):\n",
    "    \"\"\"    \n",
    "    sample_size : number of function samples\n",
    "    sample_points : observation points\n",
    "    delta : the coefficient of X^3\n",
    "    sd : the standard deviation of the observation noise\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    n_points = len(sample_points)\n",
    "    X = rng.standard_t(3, (sample_size,1)) * np.sqrt(2) * np.sin(2*np.pi*sample_points) + rng.standard_t(5, (sample_size,1)) * np.sqrt(2) * np.cos(2*np.pi*sample_points)\n",
    "    X += sample_points    # adding mean function\n",
    "    X += rng.normal(0, sd, (sample_size,n_points))    # adding noise epsilon\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating high-dimensional distributions\n",
    "\n",
    "### Generating process for different dependencies according to \n",
    "- [Zhang et al. 2018](https://link.springer.com/article/10.1007/s11222-016-9721-7) for linear dependence\n",
    "- [Pomann et al. 2016](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4812165/) with shared common coefficients of basis functions\n",
    "- [Gretton et al. 2008](https://papers.nips.cc/paper/3201-a-kernel-statistical-test-of-independence.pdf) for dependence through rotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data-generating functions\n",
    "def linear(sample_size, dim=10):\n",
    "    sample_points = np.linspace(0, 1, dim)\n",
    "    X_lin = gen_data(sample_size, sample_points, random_state=None)\n",
    "    Z_lin = np.transpose([np.random.normal(0, 1, sample_size)])\n",
    "    Y_lin = X_lin[:,[0]] + Z_lin\n",
    "    \n",
    "    return X_lin, Y_lin\n",
    "\n",
    "\n",
    "def shared_coeff(sample_size, dim=10, sd=np.sqrt(0.25), random_state=None):\n",
    "    sample_points = np.linspace(0, 1, dim)\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    n_points = len(sample_points)\n",
    "    X_basis_1 = rng.normal(0, np.sqrt(10), (sample_size,1)) * np.sqrt(2) * np.sin(2*np.pi*sample_points)\n",
    "    X_basis_2 = rng.normal(0, np.sqrt(5), (sample_size,1)) * np.sqrt(2) * np.cos(2*np.pi*sample_points)\n",
    "    Y_basis_1 = rng.normal(0, np.sqrt(10), (sample_size,1)) * np.sqrt(2) * np.sin(2*np.pi*sample_points)\n",
    "    #Y_basis_1 = X_basis_1     # common first basis function coefficients \n",
    "    #Y_basis_2 = rng.normal(0, np.sqrt(5), (sample_size,1)) * np.sqrt(2) * np.cos(2*np.pi*sample_points)\n",
    "    Y_basis_2 = X_basis_2     # common second basis function coefficients \n",
    "    \n",
    "    X = sample_points + X_basis_1 + X_basis_2 + np.random.normal(0, sd, (sample_size,n_points))\n",
    "    Y = sample_points + Y_basis_1 + Y_basis_2 + np.random.normal(0, sd, (sample_size,n_points))\n",
    "    \n",
    "    return X, Y\n",
    "\n",
    "def rotation(sample_size, dim=10, angle=0, dist='uniform', sd=np.sqrt(0.25), random_state=None):\n",
    "    sample_points = np.linspace(0, 1, dim)\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    n_points = len(sample_points)\n",
    "    \n",
    "    # step 1: define two independent distributions\n",
    "    if dist=='uniform':\n",
    "        X_mix = gen_data_uniform(sample_size, sample_points)\n",
    "        Y_mix = gen_data_uniform(sample_size, sample_points)\n",
    "    elif dist=='exp':\n",
    "        X_mix = gen_data_exp(sample_size, sample_points)\n",
    "        Y_mix = gen_data_exp(sample_size, sample_points)\n",
    "    elif dist=='studentt':\n",
    "        X_mix = gen_data_t(sample_size, sample_points)\n",
    "        Y_mix = gen_data_t(sample_size, sample_points)\n",
    "    \n",
    "    # step 2: rotate\n",
    "    theta = np.radians(angle)    # rotation angle\n",
    "    X_rot = X_mix * np.cos(theta) - Y_mix * np.sin(theta)\n",
    "    Y_rot = X_mix * np.sin(theta) + Y_mix * np.cos(theta)\n",
    "    \n",
    "    # step 3: add noise N(0, I_d)\n",
    "    X_mean = np.zeros(X_rot.shape[1])\n",
    "    X_cov = np.eye(X_rot.shape[1])\n",
    "    X_rot += np.random.multivariate_normal(X_mean, X_cov, sample_size)\n",
    "\n",
    "    Y_mean = np.zeros(Y_rot.shape[1])\n",
    "    Y_cov = np.eye(Y_rot.shape[1])\n",
    "    Y_rot += np.random.multivariate_normal(Y_mean, Y_cov, sample_size)\n",
    "    \n",
    "    # step 4: multiply by orthogonal matrix\n",
    "    if X_rot.shape[1] == 1:\n",
    "        X = X_rot\n",
    "    else:\n",
    "        X = X_rot @ ortho_group.rvs(X_rot.shape[1])\n",
    "\n",
    "    if Y_rot.shape[1] == 1:\n",
    "        Y = Y_rot\n",
    "    else:\n",
    "        Y = Y_rot @ ortho_group.rvs(Y_rot.shape[1])\n",
    "        \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating distributions over time for dependent and independent variables\n",
    "def gen_dependence(sample_size, dim, test='independence', angle=0, dist='uniform', random_state=None):\n",
    "    \n",
    "    # X and Y indepdendent\n",
    "    if test=='independence':\n",
    "        sample_points = np.linspace(0, 1, dim)\n",
    "        t1d = gen_data(sample_size, sample_points, random_state=random_state)\n",
    "        t2d = gen_data(sample_size, sample_points, random_state=random_state)  \n",
    "        \n",
    "    # Y = X1 + N(0,1)\n",
    "    elif test=='dependence_linear':\n",
    "        t1d, t2d = linear(sample_size, dim)\n",
    "        \n",
    "    # Pomann with shared basis function coefficients\n",
    "    elif test=='dependence_coeff':\n",
    "        t1d, t2d = shared_coeff(sample_size, dim)  \n",
    "   \n",
    "    elif test=='dependence_rotation':\n",
    "        t1d, t2d = rotation(sample_size, dim, angle, dist)\n",
    "    \n",
    "    return t1d, t2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Median heuristic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# median heuristic for kernel width\n",
    "def width(Z):\n",
    "    dist_mat = pairwise_distances(Z, metric='euclidean')\n",
    "    width_Z = np.median(dist_mat[dist_mat > 0])\n",
    "    \n",
    "    return width_Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------\n",
    "\n",
    "# HSIC\n",
    "\n",
    "$\\mathcal{H}_0: P_{XY} = P_X P_Y$ is a complex distribution and must be approximated. Here, we approximate it first by randomly permuting the order of $Y$ whilst the order of $X$ is kept fixed, and second by a Gamma distribution with parameters $\\alpha$ (`al`) and $\\beta$ (`bet`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HSIC with permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HSIC_permutations(X, Y, alpha, width_X, width_Y, shuffle):    # set widths to -1 for median heuristics\n",
    "    \n",
    "    m = X.shape[0]\n",
    "    \n",
    "    # median heuristics for kernel width\n",
    "    if width_X == -1:\n",
    "        width_X = width(X)      \n",
    "    if width_Y == -1:\n",
    "        width_Y = width(Y)\n",
    "    \n",
    "    # compute Gram matrices\n",
    "    K = pairwise_kernels(X, X, metric='rbf', gamma=0.5/(width_X**2))\n",
    "    L = pairwise_kernels(Y, Y, metric='rbf', gamma=0.5/(width_Y**2))\n",
    "    #KL = np.dot(K, L)\n",
    "    \n",
    "    # biased test statistic\n",
    "    # centering matrix...\n",
    "    H = np.eye(m) - (1/m) * (np.ones((m, m)))\n",
    "    \n",
    "    # ...to center K\n",
    "    K_c = H @ K @ H\n",
    "    \n",
    "    # biased statistic\n",
    "    #stat = 1/(m**2) * np.sum(np.multiply(K_c.T, L))\n",
    "    \n",
    "    # unbiased statistic\n",
    "    np.fill_diagonal(K, 0)\n",
    "    np.fill_diagonal(L, 0)\n",
    "    KL = np.dot(K, L)\n",
    "    stat = np.trace(KL)/(m*(m-3)) + np.sum(K)*np.sum(L)/(m*(m-3)*(m-1)*(m-2)) - 2*np.sum(KL)/(m*(m-3)*(m-2))\n",
    "    \n",
    "    # initiating HSIC\n",
    "    HSIC_arr = np.zeros(shuffle)\n",
    "    \n",
    "    # create permutations by reshuffling L except the main diagonal\n",
    "    for sh in range(shuffle):       \n",
    "        index_perm = np.random.permutation(L.shape[0])\n",
    "        L_perm = L[np.ix_(index_perm, index_perm)]\n",
    "        # biased\n",
    "        #HSIC_arr[sh] = 1/(m**2) * np.sum(np.multiply(K_c.T, L_perm))\n",
    "        # unbiased\n",
    "        HSIC_arr[sh] = np.trace(np.dot(K, L_perm))/(m*(m-3)) + np.sum(K)*np.sum(L_perm)/(m*(m-3)*(m-1)*(m-2)) - 2*np.sum(np.dot(K, L_perm))/(m*(m-3)*(m-2))\n",
    "        \n",
    "    HSIC_arr_sort = np.sort(HSIC_arr)\n",
    "    \n",
    "    # computing 1-alpha threshold\n",
    "    threshold = HSIC_arr_sort[round((1-alpha)*shuffle)]\n",
    "    \n",
    "    \"\"\"\n",
    "    if stat > threshold:\n",
    "        print('H0 rejected')\n",
    "    else:\n",
    "        print('H0 accepted')\n",
    "    \"\"\"\n",
    "    \n",
    "    return stat, threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HSIC with Gamma distribution approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HSIC_gamma(X, Y, alpha, width_X, width_Y):    # set widths to -1 for median heuristics\n",
    "    \n",
    "    m = X.shape[0]\n",
    "    \n",
    "    # median heuristics for kernel width\n",
    "    if width_X == -1:\n",
    "        width_X = width(X)      \n",
    "    if width_Y == -1:\n",
    "        width_Y = width(Y)\n",
    "    \n",
    "    # compute Gram matrices\n",
    "    K = pairwise_kernels(X, X, metric='rbf', gamma=0.5/(width_X**2))\n",
    "    L = pairwise_kernels(Y, Y, metric='rbf', gamma=0.5/(width_Y**2))\n",
    "    #KL = np.dot(K, L)\n",
    "    \n",
    "    # biased test statistic\n",
    "    # centering matrix...\n",
    "    H = np.eye(m) - (1/m) * (np.ones((m, m)))\n",
    "    \n",
    "    # ...to center K and L\n",
    "    K_c = H @ K @ H\n",
    "    L_c = H @ L @ H\n",
    "    \n",
    "    # biased statistic\n",
    "    #stat = 1/(m**2) * np.sum(np.multiply(K_c.T, L))\n",
    "    \n",
    "    # unbiased statistics\n",
    "    np.fill_diagonal(K, 0)\n",
    "    np.fill_diagonal(L, 0)\n",
    "    KL = np.dot(K, L)\n",
    "    stat = np.trace(KL)/(m*(m-3)) + np.sum(K)*np.sum(L)/(m*(m-1)*(m-2)*(m-3)) - 2.*np.sum(KL)/(m*(m-3)*(m-2))\n",
    "     \n",
    "    # fitting Gamma distribution to stat\n",
    "    vHSIC = np.power(1/6 * KL, 2)\n",
    "    vaHSIC = 1/(m*(m-1)) * (np.sum(vHSIC) - np.trace(vHSIC))\n",
    "    varHSIC = 72*(m-4)*(m-5)/(m*(m-1)*(m-2)*(m-3)) * vaHSIC    # variance under H0\n",
    "    \n",
    "    bone = np.ones(m)\n",
    "    \n",
    "    mu_X = 1/(m*(m-1)) * bone @ (K @ bone)\n",
    "    mu_Y = 1/(m*(m-1)) * bone @ (L @ bone)\n",
    "    \n",
    "    mHSIC = 1/m * (1 + mu_X * mu_Y - mu_X - mu_Y)    # mean under H0\n",
    "    \n",
    "    al = mHSIC**2 / varHSIC\n",
    "    bet = varHSIC * m / mHSIC\n",
    "    \n",
    "    # computing 1-alpha threshold\n",
    "    threshold = gamma.ppf(1-alpha, al, scale=bet)\n",
    "    \n",
    "    \"\"\"\n",
    "    if stat > threshold:\n",
    "        print('H0 rejected')\n",
    "    else:\n",
    "        print('H0 accepted')\n",
    "    \"\"\"\n",
    "    \n",
    "    return stat, threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Power estimation\n",
    "\n",
    "We estimate the statistical power based on 200 replications for each setting. Our experiment settings compose of various dimensions, sample sizes, mean dependencies, and variance dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimensions\n",
    "dims = [5, 10, 25, 50, 100]\n",
    "\n",
    "# dependence through rotation\n",
    "sample_sizes = [200]\n",
    "\n",
    "# angle\n",
    "angles = np.linspace(0, 45, 16)\n",
    "\n",
    "dists = ['uniform', 'exp', 'studentt']\n",
    "\n",
    "# linear dependence and shared coefficient\n",
    "sample_sizes = np.linspace(6, 40, 18)\n",
    "\n",
    "# possible tests\n",
    "tests = ['dependence_rotation', 'dependence_coeff', 'dependence_linear']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HSIC_p_m = {}\n",
    "HSIC_g_m = {}\n",
    "\n",
    "for dim in dims:\n",
    "    print('Dimensions:', dim)\n",
    "    sample_points = np.linspace(0, 1, dim)\n",
    "    \n",
    "    for sample_size in sample_sizes:\n",
    "        print('Sample size:', int(sample_size))\n",
    "        for angle in angles:\n",
    "            print('Angle:', angle)\n",
    "            for test in tests:\n",
    "                for dist in dists:\n",
    "                    print('Dist:', dist)\n",
    "        \n",
    "                    HSIC_p_m_list = []\n",
    "                    HSIC_g_m_list = []\n",
    "\n",
    "                    # repeating 200 times\n",
    "                    for i in range(200):\n",
    "\n",
    "                        # defining dependencies\n",
    "                        X, Y = gen_dependence(sample_size=int(sample_size), dim=dim, test=test, angle=angle, dist=dist, random_state=None)\n",
    "\n",
    "                        # test level alpha = 0.05, 5000 permutations\n",
    "                        HSIC_p_m_list.append(HSIC_permutations(X, Y, 0.05, -1, -1, 5000))\n",
    "\n",
    "                        # test level alpha = 0.05\n",
    "                        HSIC_g_m_list.append(HSIC_gamma(X, Y, 0.05, -1, -1))\n",
    "\n",
    "                    HSIC_p_m[(dim, int(sample_size), angle, dist)] = HSIC_p_m_list\n",
    "                    HSIC_g_m[(dim, int(sample_size), angle, dist)] = HSIC_g_m_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving\n",
    "dependence_p = open('dependencies_p.pkl', 'wb')\n",
    "pickle.dump(HSIC_p_m, dependence_p)\n",
    "dependence_p.close()\n",
    "\n",
    "dependence_g = open('dependencies_g.pkl', 'wb')\n",
    "pickle.dump(HSIC_g_m, dependence_g)\n",
    "dependence_g.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximising test power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def var_HSIC(X, Y, sigma_x, sigma_y):\n",
    "    m = X.shape[0]\n",
    "    K = pairwise_kernels(X, X, metric='rbf', gamma=0.5/(sigma_x**2))\n",
    "    L = pairwise_kernels(Y, Y, metric='rbf', gamma=0.5/(sigma_y**2))\n",
    "    np.fill_diagonal(K, 0)\n",
    "    np.fill_diagonal(L, 0)\n",
    "    KL = np.dot(K, L)\n",
    "    \n",
    "    # unbiased test statistics\n",
    "    stat = np.trace(KL)/(m*(m-3)) + np.sum(K)*np.sum(L)/(m*(m-1)*(m-2)*(m-3)) - 2*np.sum(KL)/(m*(m-3)*(m-2))\n",
    "    \n",
    "    # h and R\n",
    "    h = (m-2)**2 * np.multiply(K, L) @ np.ones((m, 1)) + (m-2) * (np.dot(np.trace(KL), np.ones((m, 1))) - 2 * np.dot(KL, np.ones((m, 1)))) - m * np.multiply(np.dot(K, np.ones((m,1))), np.dot(L, np.ones((m,1)))) + np.dot(np.sum(L), np.dot(K, np.ones((m,1)))) + np.dot(np.sum(K), np.dot(L, np.ones((m,1)))) - np.dot(np.sum(KL), np.ones((m,1)))\n",
    "    \n",
    "    R = 1 / (4*m * (m-1)**2 * (m-2)**2 * (m-3)**2) * np.dot(h.T, h)\n",
    "    \n",
    "    var_HSIC = 16/m * (R - stat**2)\n",
    "    \n",
    "    return var_HSIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maximise(HSIC, HSIC_var, threshold, m):\n",
    "    ratio = HSIC / np.sqrt(HSIC_var) - threshold / (m*np.sqrt(HSIC_var))\n",
    "    return ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Power estimation\n",
    "\n",
    "We estimate the statistical power based on 200 replications for each setting. Our experiment settings compose of various dimensions, sample sizes, mean dependencies, and variance dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimensions\n",
    "dims = [100]\n",
    "\n",
    "# sample sizes\n",
    "sample_sizes = [200]\n",
    "\n",
    "# angle\n",
    "angles = np.linspace(0, 45, 16)\n",
    "\n",
    "dists = ['uniform', 'exp', 'studentt']\n",
    "\n",
    "# possible tests\n",
    "tests = ['dependence_rotation']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Power estimation for dependence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "HSIC_p_m = {}\n",
    "HSIC_g_m = {}\n",
    "\n",
    "for dim in dims:\n",
    "    print('Dimensions:', dim)\n",
    "    sample_points = np.linspace(0, 1, dim)\n",
    "    \n",
    "    for sample_size in sample_sizes:\n",
    "        print('Sample size:', int(sample_size))\n",
    "        for angle in angles:\n",
    "            print('Angle:', angle)\n",
    "            for test in tests:\n",
    "                for dist in dists:\n",
    "                    print('Dist:', dist)\n",
    "        \n",
    "                    HSIC_p_m_list = []\n",
    "                    HSIC_g_m_list = []\n",
    "\n",
    "                    # repeating 200 times\n",
    "                    for i in range(200):\n",
    "\n",
    "                        # defining dependencies\n",
    "                        X_train, Y_train = gen_dependence(sample_size=int(sample_size), dim=dim, test=test, angle=angle, dist=dist, random_state=None)\n",
    "                        X_test, Y_test = gen_dependence(sample_size=int(sample_size), dim=dim, test=test, angle=angle, dist=dist, random_state=None)\n",
    "                        \n",
    "                        m = X_train.shape[0]\n",
    "                        \n",
    "                        if dist=='uniform':\n",
    "                            sigmas_x = np.linspace(1, 40, 40)\n",
    "                            sigmas_y = np.linspace(1, 40, 40)\n",
    "                        elif dist=='exp':\n",
    "                            sigmas_x = np.linspace(1, 20, 20)\n",
    "                            sigmas_y = np.linspace(1, 20, 20)\n",
    "                        elif dist=='studentt':\n",
    "                            sigmas_x = np.linspace(1, 20, 20)\n",
    "                            sigmas_y = np.linspace(1, 20, 20)\n",
    "                        \n",
    "                        \n",
    "                        ratios = np.empty((len(sigmas_x), len(sigmas_y)))\n",
    "                        \n",
    "                        # grid search\n",
    "                        for i, sigma_x in enumerate(sigmas_x):\n",
    "                            for j, sigma_y in enumerate(sigmas_y):\n",
    "                                HSIC, threshold = HSIC_permutations(X_train, Y_train, 0.05, sigma_x, sigma_y, 5000)\n",
    "                                HSIC_var = var_HSIC(X_train, Y_train)\n",
    "                                ratios[j,i] = maximise(HSIC, HSIC_var, threshold, m)\n",
    "\n",
    "                        # sigma of maximum ratio\n",
    "                        max_y, max_x = np.where(ratios==np.max(ratios))\n",
    "\n",
    "                        sigma_max_x = sigmas_x[max_x][0]\n",
    "                        sigma_max_y = sigmas_y[max_y][0]\n",
    "                        \n",
    "                        # test level alpha = 0.05, 5000 permutations\n",
    "                        HSIC_p_m_list.append(HSIC_permutations(X_test, Y_test, 0.05, sigma_max_x, sigma_max_y, 5000))\n",
    "\n",
    "                        # test level alpha = 0.05\n",
    "                        HSIC_g_m_list.append(HSIC_gamma(X_test, Y_test, 0.05, sigma_max_x, sigma_max_y))\n",
    "\n",
    "                    HSIC_p_m[(dim, int(sample_size), angle, dist)] = HSIC_p_m_list\n",
    "                    HSIC_g_m[(dim, int(sample_size), angle, dist)] = HSIC_g_m_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving\n",
    "dependence_p = open('dependencies_p_max.pkl', 'wb')\n",
    "pickle.dump(HSIC_p_m, dependence_p)\n",
    "dependence_p.close()\n",
    "\n",
    "dependence_g = open('dependencies_g_max.pkl', 'wb')\n",
    "pickle.dump(HSIC_g_m, dependence_g)\n",
    "dependence_g.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
